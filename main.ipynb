{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fede5fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c6ef75dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_data = 'data/data.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "32d0d9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 1. 优化后的 Feed Concentration 解析函数\n",
    "# ==============================================================================\n",
    "def get_feed_concentrations(file_path):\n",
    "    df_raw = pd.read_excel(file_path, sheet_name='feed conc')\n",
    "    \n",
    "    target_mets = {'Ala', 'Arg', 'Asn', 'Asp', 'Cys', 'Glc', 'Gln', 'Glu', 'Pyr', \n",
    "                   'Gly', 'His', 'Ile', 'Lac', 'Leu', 'Lys', 'Met', 'Nh4', 'Phe', \n",
    "                   'Pro', 'Ser', 'Thr', 'Tyr', 'Val'}\n",
    "    names = df_raw.columns.values\n",
    "    values = df_raw.iloc[1].values # (跳过单位行)\n",
    "    \n",
    "    feed_concs = {}\n",
    "    for n, v in zip(names, values):\n",
    "        if isinstance(n, str) and n.strip() in target_mets:\n",
    "            try:\n",
    "                feed_concs[n.strip()] = float(v)\n",
    "            except ValueError:\n",
    "                pass \n",
    "                \n",
    "    return feed_concs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "65344e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. 数据处理主逻辑 (计算 Mr/IR) - Simulation Ready 版\n",
    "# ==============================================================================\n",
    "def process_bioprocess_data(data_file, feed_concs):\n",
    "    # 读取数据\n",
    "    df = pd.read_excel(data_file, sheet_name='data')\n",
    "    \n",
    "    # --- 2.1 数据清洗 ---\n",
    "    if str(df.iloc[0]['Time']).strip() == 'h':\n",
    "        df = df.drop(0).reset_index(drop=True)\n",
    "        \n",
    "    cols_to_numeric = df.columns.drop(['Experiment'])\n",
    "    df[cols_to_numeric] = df[cols_to_numeric].apply(pd.to_numeric, errors='coerce')\n",
    "        \n",
    "    processed_dfs = []\n",
    "    unique_exps = df['Experiment'].unique()\n",
    "    \n",
    "    # 定义代谢物列表 (用于后续列名生成)\n",
    "    met_list = ['Ala', 'Arg', 'Asn', 'Asp', 'Cys', 'Glc', 'Gln', 'Glu', 'Pyr', \n",
    "                'Gly', 'His', 'Ile', 'Lac', 'Leu', 'Lys', 'Met', 'Nh4', 'Phe', \n",
    "                'Pro', 'Ser', 'Thr', 'Tyr', 'Val']\n",
    "    \n",
    "    print(f\"开始处理 {len(unique_exps)} 个实验批次...\")\n",
    "    \n",
    "    for exp_id in unique_exps:\n",
    "        group = df[df['Experiment'] == exp_id].copy().sort_values('Time').reset_index(drop=True)\n",
    "        \n",
    "        # --- 2.2 物理量提取 ---\n",
    "        V_L = group['V'] / 1000.0  # L\n",
    "        Feed_Vol_Incr_L = group['feed volume'] / 1000.0 \n",
    "        Sample_Vol_L = group['sample'] / 1000.0 \n",
    "        Time = group['Time'].values\n",
    "        \n",
    "        res = group[['Time', 'Experiment']].copy()\n",
    "        res['V_L'] = V_L\n",
    "        # [关键] 保存原始操作变量，用于未来模拟的输入\n",
    "        res['Feed_Vol_L'] = Feed_Vol_Incr_L\n",
    "        res['Sample_Vol_L'] = Sample_Vol_L\n",
    "        \n",
    "        # ==========================================\n",
    "        # 2.3 处理 Biomass (Xv)\n",
    "        # ==========================================\n",
    "        # Matlab 逻辑: 先算 Mcell 平衡，最后乘因子\n",
    "        X_raw = group['X'].values # Mcell/mL\n",
    "        dw_g_per_mcell = np.where(Time < 100, 2.161e-4, 2.87496e-4) # g/Mcell\n",
    "        \n",
    "        # Total Mcell\n",
    "        mass_X_mcell = X_raw * (group['V'].values) # mL * Mcell/mL = Mcell\n",
    "        \n",
    "        # Accum (Mcell)\n",
    "        accum_X_mcell = np.zeros(len(group))\n",
    "        accum_X_mcell[0] = mass_X_mcell[0] # Initial Mass\n",
    "        \n",
    "        for t in range(1, len(group)):\n",
    "            # Sample Out (Mcell) = Sample(mL) * X(Mcell/mL)\n",
    "            mass_out = group['sample'].values[t] * X_raw[t]\n",
    "            accum_X_mcell[t] = accum_X_mcell[t-1] - mass_out\n",
    "            \n",
    "        # Mr (Mcell)\n",
    "        mr_X_mcell = mass_X_mcell - accum_X_mcell\n",
    "        \n",
    "        # [调整] 保存 Accum (转换回 gDW)\n",
    "        # 注意: 这里 Accum 也需要乘以 dw 因子才能在物理层与其他质量相加减\n",
    "        # Mr(g) = V*C(g) - Accum(g)\n",
    "        res['Mr_Xv'] = mr_X_mcell * dw_g_per_mcell\n",
    "        res['Xv'] = X_raw * dw_g_per_mcell * 1000.0 # g/L\n",
    "        res['Accum_Xv'] = accum_X_mcell * dw_g_per_mcell # g\n",
    "        \n",
    "        # ==========================================\n",
    "        # 2.4 处理 mAb (Product)\n",
    "        # ==========================================\n",
    "        mAb_conc = group['mAb'].values # mg/L\n",
    "        res['Conc_mAb'] = mAb_conc\n",
    "        \n",
    "        mass_mAb = V_L * mAb_conc # mg\n",
    "        accum_mAb = np.zeros(len(group))\n",
    "        accum_mAb[0] = 0.0 # Matlab 强制归零\n",
    "        \n",
    "        for t in range(1, len(group)):\n",
    "            mass_out = Sample_Vol_L.iloc[t] * mAb_conc[t]\n",
    "            accum_mAb[t] = accum_mAb[t-1] - mass_out\n",
    "            \n",
    "        res['Mr_mAb'] = mass_mAb - accum_mAb\n",
    "        res['Accum_mAb'] = accum_mAb # mg\n",
    "        \n",
    "        # ==========================================\n",
    "        # 2.5 处理 Metabolites\n",
    "        # ==========================================\n",
    "        for met in met_list:\n",
    "            feed_c = feed_concs.get(met, 0.0)\n",
    "            \n",
    "            if met not in group.columns:\n",
    "                # 如果缺失，补全为0，防止后续Dataset读取报错\n",
    "                res[f'Conc_{met}'] = 0.0\n",
    "                res[f'Mr_{met}'] = 0.0\n",
    "                res[f'Accum_{met}'] = 0.0\n",
    "                continue\n",
    "                \n",
    "            conc = group[met].values # mM\n",
    "            \n",
    "            # Initial Accum\n",
    "            mass_0 = V_L.iloc[0] * conc[0]\n",
    "            accum_vec = np.zeros(len(group))\n",
    "            accum_vec[0] = mass_0\n",
    "            \n",
    "            for t in range(1, len(group)):\n",
    "                # Feed In (mmol) = Feed_Vol(L) * Feed_Conc(mM)\n",
    "                mass_in = Feed_Vol_Incr_L.iloc[t] * feed_c \n",
    "                # Sample Out\n",
    "                mass_out = Sample_Vol_L.iloc[t] * conc[t]\n",
    "                \n",
    "                accum_vec[t] = accum_vec[t-1] + mass_in - mass_out\n",
    "                \n",
    "            # Mr 计算\n",
    "            total_mass = V_L * conc\n",
    "            mr = total_mass - accum_vec\n",
    "            \n",
    "            res[f'Conc_{met}'] = conc\n",
    "            res[f'Mr_{met}'] = mr\n",
    "            res[f'Accum_{met}'] = accum_vec # mmol\n",
    "            \n",
    "        processed_dfs.append(res)\n",
    "\n",
    "    df_final = pd.concat(processed_dfs, ignore_index=True)\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "68caf47e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在解析 Feed 配方...\n",
      "正在计算 Accum 和 Mr...\n",
      "开始处理 9 个实验批次...\n",
      "------------------------------\n",
      "处理完成。\n",
      "输出文件: processed_data_IR_final.csv\n",
      "数据维度: (189, 80)\n",
      "包含的列 (前10个): ['Time', 'Experiment', 'V_L', 'Feed_Vol_L', 'Sample_Vol_L', 'Mr_Xv', 'Xv', 'Accum_Xv', 'Conc_mAb', 'Mr_mAb']\n",
      "------------------------------\n",
      "数据预览 (Br1, Glucose):\n",
      "   Time   Conc_Glc    Mr_Glc\n",
      "0     0  72.550000  0.000000\n",
      "1    12  65.198513 -1.827211\n",
      "2    24  56.939987 -4.015453\n",
      "3    36  48.140276 -6.477662\n",
      "4    48  39.084531 -9.140549\n"
     ]
    }
   ],
   "source": [
    "# 1. 解析 Feed\n",
    "print(\"正在解析 Feed 配方...\")\n",
    "feed_concs = get_feed_concentrations(file_data)\n",
    "\n",
    "# 2. 处理数据\n",
    "print(\"正在计算 Accum 和 Mr...\")\n",
    "df_mr = process_bioprocess_data(file_data, feed_concs)\n",
    "\n",
    "# 3. 保存\n",
    "output_csv = 'processed_data_IR_final.csv'\n",
    "df_mr.to_csv(output_csv, index=False)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"处理完成。\")\n",
    "print(f\"输出文件: {output_csv}\")\n",
    "print(f\"数据维度: {df_mr.shape}\")\n",
    "print(f\"包含的列 (前10个): {list(df_mr.columns[:10])}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# 打印前几行检验 (检查单位数量级是否合理，Mr 应该是 mmol 级别)\n",
    "print(\"数据预览 (Br1, Glucose):\")\n",
    "print(df_mr[df_mr['Experiment'] == 1][['Time', 'Conc_Glc', 'Mr_Glc']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "73a589b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature    | Max Abs Err  | Max Rel Err  | R2 Score   | Status\n",
      "-----------------------------------------------------------------\n",
      "Xv         | 4.44e-16     | 2.63e-16     | 1.0000     | ✅\n",
      "mAb        | 5.68e-14     | 2.16e-16     | 1.0000     | ✅\n",
      "Ala        | 1.78e-15     | 3.10e-16     | 1.0000     | ✅\n",
      "Arg        | 3.55e-15     | 1.27e-14     | 1.0000     | ✅\n",
      "Asn        | 4.44e-16     | 1.50e-15     | 1.0000     | ✅\n",
      "Asp        | 4.44e-16     | 3.33e-15     | 1.0000     | ✅\n",
      "Cys        | 4.44e-16     | 4.89e-15     | 1.0000     | ✅\n",
      "Glc        | 7.11e-15     | 2.34e-16     | 1.0000     | ✅\n",
      "Gln        | 3.55e-15     | 1.92e-14     | 1.0000     | ✅\n",
      "Glu        | 2.22e-16     | 4.87e-15     | 1.0000     | ✅\n",
      "Pyr        | 3.55e-15     | 1.01e-14     | 1.0000     | ✅\n",
      "Gly        | 3.55e-15     | 7.29e-16     | 1.0000     | ✅\n",
      "His        | 8.88e-16     | 1.73e-15     | 1.0000     | ✅\n",
      "Ile        | 7.11e-15     | 2.63e-14     | 1.0000     | ✅\n",
      "Lac        | 1.78e-15     | 3.98e-16     | 1.0000     | ✅\n",
      "Leu        | 3.55e-15     | 3.72e-15     | 1.0000     | ✅\n",
      "Lys        | 6.66e-16     | 7.86e-15     | 1.0000     | ✅\n",
      "Met        | 4.44e-16     | 3.29e-15     | 1.0000     | ✅\n",
      "Nh4        | 7.11e-15     | 4.24e-16     | 1.0000     | ✅\n",
      "Phe        | 3.55e-15     | 7.78e-15     | 1.0000     | ✅\n",
      "Pro        | 7.11e-15     | 2.92e-14     | 1.0000     | ✅\n",
      "Ser        | 7.11e-15     | 1.81e-15     | 1.0000     | ✅\n",
      "Thr        | 3.55e-15     | 8.32e-16     | 1.0000     | ✅\n",
      "Tyr        | 1.11e-16     | 5.13e-15     | 1.0000     | ✅\n",
      "Val        | 7.11e-15     | 1.86e-14     | 1.0000     | ✅\n"
     ]
    }
   ],
   "source": [
    "def validate_matrix(exp_id, df_mr, feature_names, org_mat_file):\n",
    "\n",
    "    \"\"\"\n",
    "    对比计算矩阵与原始矩阵，打印详细的误差统计。\n",
    "    \"\"\"\n",
    "    # Extract Exp 1\n",
    "    df_exp = df_mr[df_mr['Experiment'] == exp_id].copy()\n",
    "\n",
    "    # Total 25: Xv, mAb, + 23 mets\n",
    "    mr_cols = [col for col in df_mr.columns if col.startswith('Mr_')]\n",
    "\n",
    "    assert len(mr_cols) == 25\n",
    "\n",
    "    # Create matrix (25 x 21)\n",
    "    calculated = df_exp[mr_cols].values.T\n",
    "    original = sio.loadmat(org_mat_file)['data']['m_r'][0][exp_id-1]\n",
    "\n",
    "    n_vars = calculated.shape[0]\n",
    "    \n",
    "    print(f\"{'Feature':<10} | {'Max Abs Err':<12} | {'Max Rel Err':<12} | {'R2 Score':<10} | {'Status'}\")\n",
    "    print(\"-\" * 65)\n",
    "    \n",
    "    for i in range(n_vars):\n",
    "        y_calc = calculated[i, :]\n",
    "        y_true = original[i, :]\n",
    "        \n",
    "        # 1. 绝对误差\n",
    "        abs_diff = np.abs(y_calc - y_true)\n",
    "        max_abs_err = np.max(abs_diff)\n",
    "        \n",
    "        # 2. 相对误差 (防止除以0)\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            rel_diff = abs_diff / (np.abs(y_true) + 1e-6) # 加个小项防止除0\n",
    "            max_rel_err = np.max(rel_diff)\n",
    "            \n",
    "        # 3. R2 Score (衡量趋势一致性)\n",
    "        r2 = r2_score(y_true, y_calc)\n",
    "        \n",
    "        # 判定\n",
    "        status = \"✅\" if r2 > 0.99 else \"⚠️\"\n",
    "        if max_rel_err > 0.1 and max_abs_err > 0.1: status = \"❌\" # 误差超过10%且绝对值不忽略不计\n",
    "        \n",
    "        name = feature_names[i] if i < len(feature_names) else f\"Var_{i}\"\n",
    "        print(f\"{name:<10} | {max_abs_err:.2e}     | {max_rel_err:.2e}     | {r2:.4f}     | {status}\")\n",
    "\n",
    "# 定义变量名列表 (25个)\n",
    "metabolites_order = ['Ala', 'Arg', 'Asn', 'Asp', 'Cys', 'Glc', 'Gln', 'Glu', 'Pyr', 'Gly', 'His', 'Ile', 'Lac', 'Leu', 'Lys', 'Met', 'Nh4', 'Phe', 'Pro', 'Ser', 'Thr', 'Tyr', 'Val']\n",
    "feature_names = ['Xv', 'mAb'] + metabolites_order\n",
    "\n",
    "# 运行验证\n",
    "validate_matrix(exp_id=3, df_mr=df_mr, feature_names=feature_names, org_mat_file='data/data.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a71079e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# # ==============================================================================\n",
    "# # Step 1: 构建反应相关矩阵 S (PCA / TruncatedSVD)\n",
    "# # ==============================================================================\n",
    "# def build_s_matrix(csv_file='processed_data_IR_final.csv', n_components=7):\n",
    "#     print(f\"\\n[Step 1] Loading data from {csv_file}...\")\n",
    "#     df = pd.read_csv(csv_file)\n",
    "    \n",
    "#     # 1. 定义变量顺序 (严格对应 25 种物质)\n",
    "#     # Xv, mAb, 23种代谢物\n",
    "#     met_list = ['Ala', 'Arg', 'Asn', 'Asp', 'Cys', 'Glc', 'Gln', 'Glu', 'Pyr', \n",
    "#                 'Gly', 'His', 'Ile', 'Lac', 'Leu', 'Lys', 'Met', 'Nh4', 'Phe', \n",
    "#                 'Pro', 'Ser', 'Thr', 'Tyr', 'Val']\n",
    "    \n",
    "#     # 对应的 Mr 列名\n",
    "#     mr_cols = ['Mr_Xv', 'Mr_mAb'] + [f'Mr_{m}' for m in met_list]\n",
    "#     species_cols = ['Xv', 'Conc_mAb'] + [f'Conc_{m}' for m in met_list]\n",
    "    \n",
    "#     # 提取数据矩阵 (Samples x Features)\n",
    "#     # 将所有实验的所有时间点拼接在一起\n",
    "#     X_mr = df[mr_cols].values\n",
    "    \n",
    "#     # 2. 归一化 (Matlab逻辑: 除以最大绝对值)\n",
    "#     # 这一步是为了让不同量级的物质 (如 Glc vs Trp) 在 PCA 中具有可比性\n",
    "#     max_vals_mr = np.max(np.abs(X_mr), axis=0)\n",
    "#     max_vals_mr[max_vals_mr == 0] = 1.0 # 防止除零\n",
    "    \n",
    "#     X_norm = X_mr / max_vals_mr\n",
    "    \n",
    "#     # 3. 执行非中心化 PCA (TruncatedSVD)\n",
    "#     # Matlab: pca(..., 'Centered', false)\n",
    "#     print(f\"Executing TruncatedSVD (n_components={n_components})...\")\n",
    "#     svd = TruncatedSVD(n_components=n_components, algorithm='randomized', n_iter=10, random_state=42)\n",
    "#     svd.fit(X_norm)\n",
    "    \n",
    "#     # 4. 验证解释方差\n",
    "#     total_var = np.sum(svd.explained_variance_ratio_)\n",
    "#     print(f\"PCA Explained Variance Ratio: {total_var:.4%}\")\n",
    "#     if total_var > 0.99:\n",
    "#         print(\"✅ PCA Verification Passed (>99%)\")\n",
    "#     else:\n",
    "#         print(\"⚠️ Warning: PCA Explained Variance < 99%\")\n",
    "        \n",
    "#     # 5. 构建 S 矩阵\n",
    "#     # S = Components.T * Scaling (反归一化)\n",
    "#     # Python SVD components shape: (n_components, n_features) -> (7, 25)\n",
    "#     # S shape target: (25, 7)\n",
    "    \n",
    "#     # 这里的数学逻辑：\n",
    "#     # Data ~= Scores @ Components\n",
    "#     # Data_Real = Data_Norm * MaxVals\n",
    "#     # Data_Real ~= (Scores) @ (Components * MaxVals)\n",
    "#     # 所以 S = (Components * MaxVals).T\n",
    "    \n",
    "#     S_matrix = (svd.components_ * max_vals_mr[np.newaxis, :]).T\n",
    "#     print(f\"S Matrix Shape: {S_matrix.shape}\")\n",
    "    \n",
    "#     return S_matrix, df, species_cols, mr_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c01104b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA (7 components) 累积解释方差: 99.9357%\n",
      "S 矩阵形状: (25, 7)\n",
      "S Matrix saved to s_matrix.pkl\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. PCA 分析与 S 矩阵构建\n",
    "# ==============================================================================\n",
    "def build_reaction_correlation_matrix(csv_file, n_components=7):\n",
    "    # 读取预处理后的数据\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # 提取所有 M_r 列 (Reacted Amount)\n",
    "    met_list = ['Ala', 'Arg', 'Asn', 'Asp', 'Cys', 'Glc', 'Gln', 'Glu', 'Pyr', \n",
    "                'Gly', 'His', 'Ile', 'Lac', 'Leu', 'Lys', 'Met', 'Nh4', 'Phe', \n",
    "                'Pro', 'Ser', 'Thr', 'Tyr', 'Val']\n",
    "    \n",
    "    mr_cols = ['Mr_Xv', 'Mr_mAb'] + [f'Mr_{m}' for m in met_list]\n",
    "    species_cols = ['Xv', 'Conc_mAb'] + [f'Conc_{m}' for m in met_list]\n",
    "    \n",
    "    # 提取数据矩阵 (Samples x Features)\n",
    "    # 这里的 Samples 是所有实验的所有时间点拼接\n",
    "    X_mr = df[mr_cols].values\n",
    "    \n",
    "    # 归一化 (Normalization) - 论文方法：除以最大绝对值\n",
    "    # Matlab: reacted_masses_pca = reacted_masses ./ max(abs(reacted_masses))\n",
    "    max_vals = np.max(np.abs(X_mr), axis=0)\n",
    "    max_vals[max_vals == 0] = 1.0 # 防止除零\n",
    "    X_norm = X_mr / max_vals\n",
    "    \n",
    "    # 执行 PCA\n",
    "    pca = PCA(n_components=n_components, svd_solver='full')\n",
    "    pca.fit(X_norm)\n",
    "    \n",
    "    # 检查解释方差\n",
    "    explained_var = np.sum(pca.explained_variance_ratio_)\n",
    "    print(f\"PCA ({n_components} components) 累积解释方差: {explained_var:.4%}\")\n",
    "    \n",
    "    # 构建 S 矩阵 (Reaction Correlation Matrix)\n",
    "    # S = PCA_Components.T * Scaling_Factors\n",
    "    # 维度: (25 features, 7 components)\n",
    "    # 每一列代表一个“宏观反应” (Macro-Reaction) 的化学计量数向量\n",
    "    S_matrix = pca.components_.T * max_vals[:, np.newaxis]\n",
    "    \n",
    "    return S_matrix, df, species_cols, mr_cols\n",
    "\n",
    "# 运行\n",
    "S, df, species_cols, mr_cols = build_reaction_correlation_matrix('processed_data_IR_final.csv')\n",
    "print(\"S 矩阵形状:\", S.shape)\n",
    "\n",
    "# Save S matrix for model\n",
    "with open('s_matrix.pkl', 'wb') as f:\n",
    "    pickle.dump(S, f)\n",
    "print(\"S Matrix saved to s_matrix.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "10463774",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ==============================================================================\n",
    "# Step 2: 构建 PyTorch Dataset\n",
    "# ==============================================================================\n",
    "class BioreactorDataset(Dataset):\n",
    "    def __init__(self, df, species_cols, mr_cols, accum_cols, exp_ids, feed_concs_dict, scaler=None):\n",
    "        self.data_list = []\n",
    "        \n",
    "        # 将 feed_concs_dict 转换为 Tensor 向量 (顺序与 species_cols 对应)\n",
    "        # species_cols: ['Xv', 'mAb', 'Ala', ...]\n",
    "        # feed vector: [0, 0, C_Ala, ...]\n",
    "        feed_conc_vec = []\n",
    "        for col in species_cols:\n",
    "            met_name = col.replace('Conc_', '')\n",
    "            # Xv 和 mAb 的 Feed 浓度通常为 0\n",
    "            if met_name == 'Xv' or met_name == 'mAb' or met_name == 'Conc_mAb':\n",
    "                feed_conc_vec.append(0.0)\n",
    "            else:\n",
    "                feed_conc_vec.append(feed_concs_dict.get(met_name, 0.0))\n",
    "        \n",
    "        self.feed_conc_tensor = torch.tensor(feed_conc_vec, dtype=torch.float32)\n",
    "        \n",
    "        for exp in exp_ids:\n",
    "            group = df[df['Experiment'] == exp].sort_values('Time')\n",
    "            \n",
    "            conc = group[species_cols].values.astype(np.float32)\n",
    "            mr = group[mr_cols].values.astype(np.float32)\n",
    "            accum = group[accum_cols].values.astype(np.float32) # [新增] 直接读取计算好的 Accum\n",
    "            \n",
    "            vol = group['V_L'].values.astype(np.float32).reshape(-1, 1)\n",
    "            time = group['Time'].values.astype(np.float32).reshape(-1, 1)\n",
    "            \n",
    "            # Feed Vol & Sample Vol (用于模拟)\n",
    "            feed_vol = group['Feed_Vol_L'].values.astype(np.float32).reshape(-1, 1)\n",
    "            sample_vol = group['Sample_Vol_L'].values.astype(np.float32).reshape(-1, 1)\n",
    "            \n",
    "            inputs = np.hstack([conc, vol, time])\n",
    "            if scaler:\n",
    "                inputs = scaler.transform(inputs)\n",
    "            \n",
    "            self.data_list.append({\n",
    "                'inputs': torch.tensor(inputs),\n",
    "                'mr': torch.tensor(mr),\n",
    "                'accum': torch.tensor(accum), # 用于训练时的物理约束\n",
    "                'conc': torch.tensor(conc),\n",
    "                'vol': torch.tensor(vol),\n",
    "                'time': torch.tensor(time),\n",
    "                'feed_vol': torch.tensor(feed_vol), # 模拟输入\n",
    "                'sample_vol': torch.tensor(sample_vol), # 模拟输入\n",
    "                'feed_conc': self.feed_conc_tensor # 常数向量\n",
    "            })\n",
    "            \n",
    "    def __len__(self): return len(self.data_list)\n",
    "    def __getitem__(self, idx): return self.data_list[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ca28774f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def prepare_dataloaders(df, species_cols, mr_cols, feed_concs_dict):\n",
    "    \"\"\"\n",
    "    准备 PyTorch DataLoader，支持训练（Accum GT）和模拟（Feed Strategy）。\n",
    "    \n",
    "    Args:\n",
    "        df: 包含所有数据的 DataFrame\n",
    "        species_cols: 浓度列名列表 ['Xv', 'Conc_mAb', 'Conc_Glc', ...]\n",
    "        mr_cols: 反应量列名列表 ['Mr_Xv', 'Mr_mAb', 'Mr_Glc', ...]\n",
    "        feed_concs_dict: Feed 浓度字典 {'Glc': 42.8, ...}\n",
    "    \"\"\"\n",
    "    print(f\"\\n[Step 2] Preparing PyTorch Datasets...\")\n",
    "    \n",
    "    # 0. 自动生成 Accum 列名列表 (必须与 species_cols 顺序一一对应)\n",
    "    # 规则：Xv -> Accum_Xv, Conc_mAb -> Accum_mAb, Conc_Glc -> Accum_Glc\n",
    "    accum_cols = []\n",
    "    for col in species_cols:\n",
    "        if col == 'Xv':\n",
    "            accum_cols.append('Accum_Xv')\n",
    "        elif col == 'Conc_mAb':\n",
    "            accum_cols.append('Accum_mAb')\n",
    "        else:\n",
    "            # 假设其他列都是 'Conc_MetName' 格式\n",
    "            accum_cols.append(col.replace('Conc_', 'Accum_'))\n",
    "            \n",
    "    # 简单校验：确保这些列在 df 中存在\n",
    "    missing_accum = [c for c in accum_cols if c not in df.columns]\n",
    "    if missing_accum:\n",
    "        raise ValueError(f\"DataFrame 中缺失以下 Accum 列: {missing_accum}\\n请检查 process_bioprocess_data 是否正确保存了 Accum。\")\n",
    "\n",
    "    # 1. 划分数据集 (Fixed Split consistent with Matlab)\n",
    "    train_exps = [1, 2, 3, 4, 9]\n",
    "    val_exps = [7]\n",
    "    test_exps = [5, 6, 8]\n",
    "    \n",
    "    print(f\"Train Exps: {train_exps}\")\n",
    "    print(f\"Val Exps:   {val_exps}\")\n",
    "    print(f\"Test Exps:  {test_exps}\")\n",
    "    \n",
    "    # 2. 计算归一化统计量 (Fit Scaler on Train Data ONLY)\n",
    "    # 提取所有训练数据用于计算均值和方差\n",
    "    train_df = df[df['Experiment'].isin(train_exps)]\n",
    "    \n",
    "    # Input columns: Species(25) + Vol(1) + Time(1) = 27\n",
    "    input_data = []\n",
    "    for exp in train_exps:\n",
    "        group = train_df[train_df['Experiment'] == exp].sort_values('Time')\n",
    "        conc = group[species_cols].values\n",
    "        vol = group['V_L'].values.reshape(-1, 1)\n",
    "        time = group['Time'].values.reshape(-1, 1)\n",
    "        input_data.append(np.hstack([conc, vol, time]))\n",
    "        \n",
    "    all_train_inputs = np.vstack(input_data)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(all_train_inputs)\n",
    "    print(\"Scaler fitted on training data.\")\n",
    "    \n",
    "    # 3. 构建 Dataset 对象 (传入 accum_cols 和 feed_concs_dict)\n",
    "    # 注意：这里的 BioreactorDataset 类必须是你刚才更新过的版本\n",
    "    train_dataset = BioreactorDataset(df, species_cols, mr_cols, accum_cols, train_exps, feed_concs_dict, scaler)\n",
    "    val_dataset = BioreactorDataset(df, species_cols, mr_cols, accum_cols, val_exps, feed_concs_dict, scaler)\n",
    "    test_dataset = BioreactorDataset(df, species_cols, mr_cols, accum_cols, test_exps, feed_concs_dict, scaler)\n",
    "    \n",
    "    # 4. 构建 DataLoader\n",
    "    # batch_size = 4 (Mini-batch training)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False) # Val/Test 通常按序列逐个评估\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0089d712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Step 2] Preparing PyTorch Datasets...\n",
      "Train Exps: [1, 2, 3, 4, 9]\n",
      "Val Exps:   [7]\n",
      "Test Exps:  [5, 6, 8]\n",
      "Scaler fitted on training data.\n",
      "\n",
      "[Verification] Checking DataLoader Output:\n",
      "Input Batch Shape: torch.Size([4, 21, 27]) (Batch, Seq, Features)\n",
      "Target Batch Shape: torch.Size([4, 21, 25]) (Batch, Seq, Features)\n",
      "✅ Step 2 Verification Passed: Dimensions correct (In=27, Out=25)\n"
     ]
    }
   ],
   "source": [
    "# Run Step 2\n",
    "train_loader, val_loader, test_loader, scaler = prepare_dataloaders(df, species_cols, mr_cols, feed_concs)\n",
    "\n",
    "\n",
    "def validate_dataloader(train_loader):\n",
    "    # Verification\n",
    "    print(\"\\n[Verification] Checking DataLoader Output:\")\n",
    "    sample_batch = next(iter(train_loader))\n",
    "    inputs = sample_batch['inputs']\n",
    "    targets = sample_batch['mr']\n",
    "\n",
    "    print(f\"Input Batch Shape: {inputs.shape} (Batch, Seq, Features)\")\n",
    "    print(f\"Target Batch Shape: {targets.shape} (Batch, Seq, Features)\")\n",
    "\n",
    "    if inputs.shape[2] == 27 and targets.shape[2] == 25:\n",
    "        print(\"✅ Step 2 Verification Passed: Dimensions correct (In=27, Out=25)\")\n",
    "    else:\n",
    "        print(\"❌ Step 2 Verification Failed: Dimensions incorrect\")\n",
    "        \n",
    "validate_dataloader(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c6249b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pickle\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. 自定义加权损失函数 (Weighted MSE - Paper Eq S1.3)\n",
    "# ==============================================================================\n",
    "class WeightedMSELoss(nn.Module):\n",
    "    def __init__(self, max_vals, device):\n",
    "        super().__init__()\n",
    "        # 论文中使用 1/sigma^2 作为权重\n",
    "        # Matlab 代码实际上使用了 1/max^2 (归一化 MSE)\n",
    "        # 这里 max_vals 是 25 个物种的最大浓度值\n",
    "        self.weights = 1.0 / (torch.tensor(max_vals, dtype=torch.float32).to(device) ** 2)\n",
    "        # 防止权重过大 (比如 max=0 的情况)\n",
    "        self.weights = torch.clamp(self.weights, max=1e6)\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        # pred, target: (Batch, Seq, 25)\n",
    "        diff = (pred - target) ** 2\n",
    "        # 按特征维度加权求和，然后对 Batch 和 Seq 取平均\n",
    "        weighted_diff = diff * self.weights\n",
    "        return torch.mean(weighted_diff)\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. 混合神经 ODE 模型 (Hybrid LSTM)\n",
    "# ==============================================================================\n",
    "class HybridLSTM(nn.Module):\n",
    "    def __init__(self, S_matrix, input_dim=27, hidden_dim=16, latent_dim=7, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        \n",
    "        # [关键] 冻结的 S 矩阵 (25, 7)\n",
    "        self.S = torch.tensor(S_matrix, dtype=torch.float32).to(device)\n",
    "        self.register_buffer('S_const', self.S) # 不参与梯度更新\n",
    "        \n",
    "        # 神经网络: In(27) -> ReLU(16) -> LSTM(7) -> Linear(7)\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.lstm = nn.LSTM(input_size=hidden_dim, hidden_size=latent_dim, batch_first=True)\n",
    "        self.fc_out = nn.Linear(latent_dim, latent_dim) # Output: 7 Scores\n",
    "        \n",
    "    def forward(self, x_seq):\n",
    "        \"\"\"\n",
    "        x_seq: (Batch, Seq, 27)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x_seq.shape\n",
    "        \n",
    "        # 1. NN 前向传播\n",
    "        # Flatten for Linear\n",
    "        x_flat = x_seq.reshape(-1, x_seq.size(2))\n",
    "        features = self.encoder(x_flat)\n",
    "        features = features.reshape(batch_size, seq_len, -1)\n",
    "        \n",
    "        # LSTM\n",
    "        lstm_out, _ = self.lstm(features)\n",
    "        \n",
    "        # Output Scores (Batch, Seq, 7)\n",
    "        scores = self.fc_out(lstm_out)\n",
    "        \n",
    "        # 2. 混合层: 映射回 25 种反应增量\n",
    "        # dMr = Scores @ S.T\n",
    "        # (Batch, Seq, 7) @ (7, 25) -> (Batch, Seq, 25)\n",
    "        delta_mr_reaction = torch.matmul(scores, self.S.t())\n",
    "        \n",
    "        return delta_mr_reaction\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. 物理层与训练器\n",
    "# ==============================================================================\n",
    "class HybridTrainer:\n",
    "    def __init__(self, model, train_loader, val_loader, max_conc_vals, scaler):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = model.to(self.device)\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        \n",
    "        # 优化器 (Adam, lr=0.001 as per paper)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
    "        \n",
    "        # 损失函数\n",
    "        self.criterion = WeightedMSELoss(max_conc_vals, self.device)\n",
    "        \n",
    "        # 归一化工具 (用于反归一化 Model Input)\n",
    "        self.scaler_mean = torch.tensor(scaler.mean_, dtype=torch.float32).to(self.device)\n",
    "        self.scaler_scale = torch.tensor(scaler.scale_, dtype=torch.float32).to(self.device)\n",
    "        \n",
    "        self.history = {'train_loss': [], 'val_loss': []}\n",
    "\n",
    "    def physics_step(self, mr_curr, dMr_pred, accum_next, v_next):\n",
    "            \"\"\"\n",
    "            物理更新方程 (基于质量守恒)\n",
    "            \n",
    "            Args:\n",
    "                mr_curr: 当前时刻的反应累积量 (Batch, 25)\n",
    "                dMr_pred: NN预测的本步反应增量 (Batch, 25)\n",
    "                accum_next: 下一时刻的物理累积量 (Batch, 25) [来自Dataset]\n",
    "                            Accum = Sum(Feed_in) - Sum(Sample_out)\n",
    "                v_next: 下一时刻的体积 (Batch, 1)\n",
    "                \n",
    "            Returns:\n",
    "                c_next: 下一时刻的浓度 (用于作为下一步NN的输入)\n",
    "                mr_next: 下一时刻的反应累积量\n",
    "            \"\"\"\n",
    "            # 1. 更新反应累积量 (Reacted Mass State Update)\n",
    "            mr_next = mr_curr + dMr_pred\n",
    "            \n",
    "            # 2. 计算总质量 (Total Mass = Reacted + Physical_Accumulated)\n",
    "            # 这是论文 S1.3 公式的等价形式，但数值上更稳定\n",
    "            total_mass_next = mr_next + accum_next\n",
    "            \n",
    "            # 3. 计算浓度 (Concentration = Mass / Vol)\n",
    "            c_next = total_mass_next / v_next\n",
    "            \n",
    "            return c_next, mr_next\n",
    "\n",
    "    def train_epoch(self):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in self.train_loader:\n",
    "            # 数据移动到 GPU\n",
    "            inputs = batch['inputs'].to(self.device) # (B, T, 27) Scaled\n",
    "            target_mr = batch['mr'].to(self.device)  # (B, T, 25) Real Mass\n",
    "            \n",
    "            # 前向传播 (一次性算出整个序列的 dMr)\n",
    "            # 输出: (B, T, 25) - 每个时间步的反应增量\n",
    "            pred_dMr_seq = self.model(inputs)\n",
    "            \n",
    "            # --- 序列累积误差计算 ---\n",
    "            # 我们希望 Pred_Mr(t+1) 接近 Target_Mr(t+1)\n",
    "            # Pred_Mr(t+1) = Target_Mr(t) + Pred_dMr(t)\n",
    "            # 这是一种 \"Teacher Forcing\" 策略 (使用真实的上一步作为基准)\n",
    "            \n",
    "            # 取 t=0 到 t=T-1 的 Target 作为基准\n",
    "            mr_curr = target_mr[:, :-1, :]\n",
    "            # 取 t=1 到 t=T 的 Target 作为目标\n",
    "            mr_next_target = target_mr[:, 1:, :]\n",
    "            \n",
    "            # 对应的预测增量 (去掉最后一步的预测，因为没有 Target)\n",
    "            dMr_pred = pred_dMr_seq[:, :-1, :]\n",
    "            \n",
    "            # 预测的下一步 Mr\n",
    "            mr_next_pred = mr_curr + dMr_pred\n",
    "            \n",
    "            # 计算 Loss (直接在 Reacted Mass 上计算 WMSE)\n",
    "            loss = self.criterion(mr_next_pred, mr_next_target)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        return total_loss / len(self.train_loader)\n",
    "\n",
    "    def validate(self):\n",
    "        self.model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in self.val_loader:\n",
    "                inputs = batch['inputs'].to(self.device)\n",
    "                target_mr = batch['mr'].to(self.device)\n",
    "                \n",
    "                pred_dMr_seq = self.model(inputs)\n",
    "                \n",
    "                mr_curr = target_mr[:, :-1, :]\n",
    "                mr_next_target = target_mr[:, 1:, :]\n",
    "                dMr_pred = pred_dMr_seq[:, :-1, :]\n",
    "                \n",
    "                mr_next_pred = mr_curr + dMr_pred\n",
    "                loss = self.criterion(mr_next_pred, mr_next_target)\n",
    "                val_loss += loss.item()\n",
    "        return val_loss / len(self.val_loader)\n",
    "\n",
    "    def fit(self, epochs=200, patience=20):\n",
    "        print(f\"Starting training on {self.device}...\")\n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        best_model_wts = copy.deepcopy(self.model.state_dict())\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            train_loss = self.train_epoch()\n",
    "            val_loss = self.validate()\n",
    "            \n",
    "            self.history['train_loss'].append(train_loss)\n",
    "            self.history['val_loss'].append(val_loss)\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch {epoch:3d} | Train Loss: {train_loss:.6f} | Val Loss: {val_loss:.6f}\")\n",
    "            \n",
    "            # Checkpoint\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model_wts = copy.deepcopy(self.model.state_dict())\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                \n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "                \n",
    "        # Load best weights\n",
    "        self.model.load_state_dict(best_model_wts)\n",
    "        return self.history\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. 执行训练验证\n",
    "# ==============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    # 加载 Step 1 的结果\n",
    "    with open('s_matrix.pkl', 'rb') as f:\n",
    "        S_matrix = pickle.load(f)\n",
    "    \n",
    "    # 重新加载 Step 2 的 DataLoader (如果变量不在内存中)\n",
    "    # 假设 df, species_cols, mr_cols 已经在内存中，否则需要重新生成\n",
    "    # 这里为了演示，直接使用上面的 train_loader\n",
    "    \n",
    "    # 获取用于 Loss 归一化的 Max Values (从 scaler 中估算或重新计算)\n",
    "    # 这里我们简单重新计算一下 Training Set 的 Max Mr\n",
    "    train_df = df[df['ExperimentID'].isin([1, 2, 3, 4, 9])]\n",
    "    max_mr_vals = np.max(np.abs(train_df[mr_cols].values), axis=0)\n",
    "    max_mr_vals[max_mr_vals==0] = 1.0\n",
    "    \n",
    "    # 初始化模型\n",
    "    model = HybridLSTM(S_matrix, input_dim=27, latent_dim=7)\n",
    "    \n",
    "    # 初始化训练器\n",
    "    trainer = HybridTrainer(model, train_loader, val_loader, max_mr_vals, scaler)\n",
    "    \n",
    "    # 开始训练\n",
    "    history = trainer.fit(epochs=200)\n",
    "    \n",
    "    # 绘制 Loss\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history['train_loss'], label='Train Loss')\n",
    "    plt.plot(history['val_loss'], label='Val Loss')\n",
    "    plt.yscale('log')\n",
    "    plt.title('Hybrid Model Training (WMSE)')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"✅ Step 3 & 4 Verification Passed: Model trained and converged.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-modeling-of-bioreactor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
